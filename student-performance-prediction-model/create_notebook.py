#!/usr/bin/env python3
"""
Script to create a Jupyter notebook for model accuracy analysis
"""

import json
import os

def create_notebook():
    """Create the Jupyter notebook with proper structure"""

    notebook_content = {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "# üìä Student Performance Prediction Model - Accuracy Analysis\n",
                    "\n",
                    "**Date**: January 6, 2026  \n",
                    "**Model**: Linear Regression for Student Performance Prediction  \n",
                    "**Purpose**: Analyze and visualize the accuracy of the trained model\n",
                    "\n",
                    "---\n",
                    "\n",
                    "## üéØ Overview\n",
                    "\n",
                    "This notebook analyzes the accuracy of the Student Performance Prediction Model by:\n",
                    "\n",
                    "1. **Loading the trained model** and test data\n",
                    "2. **Making predictions** on unseen test data\n",
                    "3. **Calculating accuracy metrics** (R¬≤, MAE, RMSE, etc.)\n",
                    "4. **Creating visualizations** to understand model performance\n",
                    "5. **Identifying areas for improvement**\n",
                    "\n",
                    "---\n",
                    "\n",
                    "## üìã Table of Contents\n",
                    "\n",
                    "1. [Setup and Imports](#setup)\n",
                    "2. [Load Model and Data](#load-data)\n",
                    "3. [Make Predictions](#predictions)\n",
                    "4. [Accuracy Metrics](#metrics)\n",
                    "5. [Visualizations](#visualizations)\n",
                    "6. [Analysis and Conclusions](#analysis)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 1. Setup and Imports\n",
                    "\n",
                    "Import all required libraries for model evaluation and visualization."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Import required libraries\n",
                    "import pandas as pd\n",
                    "import numpy as np\n",
                    "import matplotlib.pyplot as plt\n",
                    "import seaborn as sns\n",
                    "import joblib\n",
                    "import os\n",
                    "import sys\n",
                    "from sklearn.model_selection import train_test_split\n",
                    "from sklearn.metrics import (\n",
                    "    mean_absolute_error, \n",
                    "    mean_squared_error, \n",
                    "    r2_score,\n",
                    "    mean_absolute_percentage_error\n",
                    ")\n",
                    "\n",
                    "# Add parent directory to path for config imports\n",
                    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
                    "\n",
                    "# Set up plotting style\n",
                    "plt.style.use('seaborn-v0_8')\n",
                    "sns.set_palette(\"husl\")\n",
                    "plt.rcParams['figure.figsize'] = (12, 8)\n",
                    "plt.rcParams['font.size'] = 12\n",
                    "\n",
                    "print(\"‚úÖ All libraries imported successfully!\")\n",
                    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
                    "print(f\"üìà NumPy version: {np.__version__}\")\n",
                    "print(f\"üìâ Seaborn version: {sns.__version__}\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 2. Load Model and Data\n",
                    "\n",
                    "Load the trained model, scaler, and test dataset for evaluation."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Configuration paths\n",
                    "MODEL_DIR = 'models'\n",
                    "DATA_DIR = 'data'\n",
                    "CONFIG_DIR = 'config'\n",
                    "\n",
                    "# File paths\n",
                    "MODEL_PATH = os.path.join(MODEL_DIR, 'performance_predictor.pkl')\n",
                    "SCALER_PATH = os.path.join(MODEL_DIR, 'scaler.pkl')\n",
                    "ENCODER_PATH = os.path.join(MODEL_DIR, 'label_encoder.pkl')\n",
                    "DATA_PATH = os.path.join(DATA_DIR, 'cleaned_data.csv')\n",
                    "\n",
                    "print(\"üîç Checking file paths...\")\n",
                    "print(f\"Model path: {MODEL_PATH}\")\n",
                    "print(f\"Scaler path: {SCALER_PATH}\")\n",
                    "print(f\"Encoder path: {ENCODER_PATH}\")\n",
                    "print(f\"Data path: {DATA_PATH}\")\n",
                    "\n",
                    "# Check if files exist\n",
                    "files_exist = {\n",
                    "    'Model': os.path.exists(MODEL_PATH),\n",
                    "    'Scaler': os.path.exists(SCALER_PATH),\n",
                    "    'Encoder': os.path.exists(ENCODER_PATH),\n",
                    "    'Data': os.path.exists(DATA_PATH)\n",
                    "}\n",
                    "\n",
                    "for file_name, exists in files_exist.items():\n",
                    "    status = \"‚úÖ Found\" if exists else \"‚ùå Missing\"\n",
                    "    print(f\"{file_name}: {status}\")\n",
                    "\n",
                    "if not all(files_exist.values()):\n",
                    "    print(\"\\n‚ö†Ô∏è  Some files are missing. Please ensure you have:\")\n",
                    "    print(\"1. Trained model (run: python src/model_trainer.py)\")\n",
                    "    print(\"2. Cleaned dataset (run: python src/data_preprocessing.py)\")\n",
                    "    raise FileNotFoundError(\"Required files not found\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Load the trained model and preprocessing objects\n",
                    "print(\"üîÑ Loading trained model and preprocessing objects...\")\n",
                    "\n",
                    "try:\n",
                    "    # Load model\n",
                    "    model = joblib.load(MODEL_PATH)\n",
                    "    print(f\"‚úÖ Model loaded: {type(model).__name__}\")\n",
                    "    \n",
                    "    # Load scaler\n",
                    "    scaler = joblib.load(SCALER_PATH)\n",
                    "    print(f\"‚úÖ Scaler loaded: {type(scaler).__name__}\")\n",
                    "    \n",
                    "    # Load label encoder\n",
                    "    label_encoder = joblib.load(ENCODER_PATH)\n",
                    "    print(f\"‚úÖ Label encoder loaded: {type(label_encoder).__name__}\")\n",
                    "    \n",
                    "    print(f\"üìö Subject encoding: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
                    "    \n",
                    "except Exception as e:\n",
                    "    print(f\"‚ùå Error loading model files: {e}\")\n",
                    "    raise"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Load and prepare the dataset\n",
                    "print(\"üîÑ Loading and preparing dataset...\")\n",
                    "\n",
                    "try:\n",
                    "    # Load data\n",
                    "    df = pd.read_csv(DATA_PATH)\n",
                    "    print(f\"‚úÖ Dataset loaded: {len(df)} records, {len(df.columns)} columns\")\n",
                    "    \n",
                    "    # Display basic info\n",
                    "    print(\"\\nüìä Dataset Overview:\")\n",
                    "    print(f\"Columns: {list(df.columns)}\")\n",
                    "    print(f\"Data types:\\n{df.dtypes}\")\n",
                    "    \n",
                    "    # Basic statistics\n",
                    "    print(\"\\nüìà Basic Statistics:\")\n",
                    "    print(df.describe())\n",
                    "    \n",
                    "except Exception as e:\n",
                    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
                    "    raise"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Prepare features for evaluation\n",
                    "print(\"üîÑ Preparing features for evaluation...\")\n",
                    "\n",
                    "# Encode subjects\n",
                    "df['subject_encoded'] = label_encoder.transform(df['subject'])\n",
                    "\n",
                    "# Define features\n",
                    "feature_columns = ['age', 'grade', 'attendance', 'marks', 'subject_encoded']\n",
                    "target_column = 'future_performance'\n",
                    "\n",
                    "# Split into features and target\n",
                    "X = df[feature_columns].values\n",
                    "y = df[target_column].values\n",
                    "\n",
                    "print(f\"‚úÖ Features prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n",
                    "print(f\"‚úÖ Target prepared: {y.shape[0]} samples\")\n",
                    "\n",
                    "# Split data (same parameters as training)\n",
                    "TEST_SIZE = 0.2\n",
                    "RANDOM_STATE = 42\n",
                    "\n",
                    "X_train, X_test, y_train, y_test = train_test_split(\n",
                    "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
                    ")\n",
                    "\n",
                    "print(\"\\nüìä Data Split:\")\n",
                    "print(f\"Training set: {len(X_train)} samples ({(1-TEST_SIZE)*100:.0f}%)\")\n",
                    "print(f\"Test set: {len(X_test)} samples ({TEST_SIZE*100:.0f}%)\")\n",
                    "\n",
                    "# Scale features\n",
                    "X_test_scaled = scaler.transform(X_test)\n",
                    "print(\"‚úÖ Features scaled for prediction\")"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 3. Make Predictions\n",
                    "\n",
                    "Use the trained model to make predictions on the test dataset."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Make predictions\n",
                    "print(\"üîÑ Making predictions on test data...\")\n",
                    "\n",
                    "try:\n",
                    "    y_pred = model.predict(X_test_scaled)\n",
                    "    print(f\"‚úÖ Predictions completed: {len(y_pred)} predictions generated\")\n",
                    "    \n",
                    "    # Basic prediction statistics\n",
                    "    print(\"\\nüìä Prediction Statistics:\")\n",
                    "    print(f\"Actual values range: {y_test.min():.2f} - {y_test.max():.2f}\")\n",
                    "    print(f\"Predicted values range: {y_pred.min():.2f} - {y_pred.max():.2f}\")\n",
                    "    print(f\"Average actual: {y_test.mean():.2f}\")\n",
                    "    print(f\"Average predicted: {y_pred.mean():.2f}\")\n",
                    "    \n",
                    "except Exception as e:\n",
                    "    print(f\"‚ùå Error making predictions: {e}\")\n",
                    "    raise"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Create a DataFrame with actual vs predicted values\n",
                    "results_df = pd.DataFrame({\n",
                    "    'actual': y_test,\n",
                    "    'predicted': y_pred,\n",
                    "    'error': y_test - y_pred,\n",
                    "    'abs_error': np.abs(y_test - y_pred)\n",
                    "})\n",
                    "\n",
                    "# Add performance categories\n",
                    "results_df['performance_category'] = pd.cut(\n",
                    "    results_df['actual'], \n",
                    "    bins=[0, 40, 60, 80, 100], \n",
                    "    labels=['Poor', 'Below Average', 'Average', 'Excellent']\n",
                    ")\n",
                    "\n",
                    "print(\"‚úÖ Results DataFrame created\")\n",
                    "print(f\"Shape: {results_df.shape}\")\n",
                    "print(\"\\nüìä Sample Results:\")\n",
                    "print(results_df.head(10))"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 4. Accuracy Metrics\n",
                    "\n",
                    "Calculate comprehensive accuracy metrics to evaluate model performance."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Calculate comprehensive accuracy metrics\n",
                    "print(\"üîÑ Calculating accuracy metrics...\")\n",
                    "\n",
                    "# Regression metrics\n",
                    "mae = mean_absolute_error(y_test, y_pred)\n",
                    "mse = mean_squared_error(y_test, y_pred)\n",
                    "rmse = np.sqrt(mse)\n",
                    "r2 = r2_score(y_test, y_pred)\n",
                    "mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
                    "\n",
                    "# Adjusted R¬≤\n",
                    "n = len(y_test)\n",
                    "p = len(feature_columns)\n",
                    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
                    "\n",
                    "# Custom accuracy metrics\n",
                    "threshold_5 = np.mean(np.abs(y_test - y_pred) <= 5.0) * 100\n",
                    "threshold_10 = np.mean(np.abs(y_test - y_pred) <= 10.0) * 100\n",
                    "threshold_15 = np.mean(np.abs(y_test - y_pred) <= 15.0) * 100\n",
                    "\n",
                    "# Create metrics dictionary\n",
                    "metrics = {\n",
                    "    'mae': mae,\n",
                    "    'mse': mse,\n",
                    "    'rmse': rmse,\n",
                    "    'r2_score': r2,\n",
                    "    'adjusted_r2': adjusted_r2,\n",
                    "    'mape': mape,\n",
                    "    'accuracy_within_5': threshold_5,\n",
                    "    'accuracy_within_10': threshold_10,\n",
                    "    'accuracy_within_15': threshold_15,\n",
                    "    'total_predictions': n\n",
                    "}\n",
                    "\n",
                    "print(\"‚úÖ All metrics calculated\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Display accuracy metrics in a formatted way\n",
                    "print(\"üìä\" + \"=\"*60)\n",
                    "print(\"MODEL ACCURACY METRICS\")\n",
                    "print(\"=\"*60)\n",
                    "\n",
                    "print(\"\\nüìà ACCURACY METRICS:\")\n",
                    "print(\"-\"*60)\n",
                    "print(f\"  R¬≤ Score (Coefficient of Determination):  {metrics['r2_score']:.4f} ({metrics['r2_score']*100:.2f}%)\")\n",
                    "print(f\"  Adjusted R¬≤ Score:                         {metrics['adjusted_r2']:.4f} ({metrics['adjusted_r2']*100:.2f}%)\")\n",
                    "print(f\"  Accuracy within ¬±5 points:                 {metrics['accuracy_within_5']:.2f}%\")\n",
                    "print(f\"  Accuracy within ¬±10 points:                {metrics['accuracy_within_10']:.2f}%\")\n",
                    "\n",
                    "print(\"üìè ERROR METRICS:\")\n",
                    "print(\"-\"*60)\n",
                    "print(f\"  Mean Absolute Error (MAE):                 {metrics['mae']:.4f} points\")\n",
                    "print(f\"  Root Mean Squared Error (RMSE):            {metrics['rmse']:.4f} points\")\n",
                    "print(f\"  Mean Absolute Percentage Error (MAPE):     {metrics['mape']:.2f}%\")\n",
                    "\n",
                    "print(\"üìà PREDICTION STATISTICS:\")\n",
                    "print(\"-\"*60)\n",
                    "print(f\"  Total test predictions:                    {metrics['total_predictions']}\")\n",
                    "print(f\"  Actual values range:                       {y_test.min():.2f} - {y_test.max():.2f}\")\n",
                    "print(f\"  Predicted values range:                    {y_pred.min():.2f} - {y_pred.max():.2f}\")\n",
                    "print(f\"  Average prediction error:                  ¬±{metrics['mae']:.2f} points\")\n",
                    "\n",
                    "# Performance interpretation\n",
                    "print(\"\\nüí° INTERPRETATION:\")\n",
                    "print(\"-\"*60)\n",
                    "\n",
                    "if metrics['r2_score'] >= 0.9:\n",
                    "    r2_quality = \"Excellent\"\n",
                    "elif metrics['r2_score'] >= 0.8:\n",
                    "    r2_quality = \"Very Good\"\n",
                    "elif metrics['r2_score'] >= 0.7:\n",
                    "    r2_quality = \"Good\"\n",
                    "elif metrics['r2_score'] >= 0.5:\n",
                    "    r2_quality = \"Moderate\"\n",
                    "else:\n",
                    "    r2_quality = \"Needs Improvement\"\n",
                    "\n",
                    "print(f\"  R¬≤ Score Quality: {r2_quality}\")\n",
                    "print(f\"  The model explains {metrics['r2_score']*100:.1f}% of the variance in student performance.\")\n",
                    "\n",
                    "if metrics['accuracy_within_5'] >= 80:\n",
                    "    print(f\"  ‚úÖ High accuracy: {metrics['accuracy_within_5']:.1f}% of predictions are within ¬±5 points\")\n",
                    "elif metrics['accuracy_within_5'] >= 60:\n",
                    "    print(f\"  ~ Moderate accuracy: {metrics['accuracy_within_5']:.1f}% of predictions are within ¬±5 points\")\n",
                    "else:\n",
                    "    print(f\"  ‚ö† Low accuracy: Only {metrics['accuracy_within_5']:.1f}% of predictions are within ¬±5 points\")\n",
                    "\n",
                    "print(f\"  Average prediction is off by {metrics['mae']:.2f} points from actual performance.\")\n",
                    "print(\"=\"*60)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 5. Visualizations\n",
                    "\n",
                    "Create comprehensive visualizations to understand model performance."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Create a comprehensive visualization dashboard\n",
                    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
                    "fig.suptitle('Student Performance Prediction Model - Accuracy Analysis', fontsize=16, fontweight='bold')\n",
                    "\n",
                    "# Plot 1: Actual vs Predicted Scatter Plot\n",
                    "axes[0, 0].scatter(y_test, y_pred, alpha=0.6, color='blue', edgecolors='black', linewidth=0.5)\n",
                    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
                    "axes[0, 0].set_xlabel('Actual Performance')\n",
                    "axes[0, 0].set_ylabel('Predicted Performance')\n",
                    "axes[0, 0].set_title('Actual vs Predicted Performance')\n",
                    "axes[0, 0].legend()\n",
                    "axes[0, 0].grid(True, alpha=0.3)\n",
                    "\n",
                    "# Plot 2: Residual Plot\n",
                    "residuals = y_test - y_pred\n",
                    "axes[0, 1].scatter(y_pred, residuals, alpha=0.6, color='green', edgecolors='black', linewidth=0.5)\n",
                    "axes[0, 1].axhline(y=0, color='red', linestyle='--', lw=2)\n",
                    "axes[0, 1].set_xlabel('Predicted Performance')\n",
                    "axes[0, 1].set_ylabel('Residuals (Actual - Predicted)')\n",
                    "axes[0, 1].set_title('Residual Plot')\n",
                    "axes[0, 1].grid(True, alpha=0.3)\n",
                    "\n",
                    "# Plot 3: Error Distribution\n",
                    "axes[0, 2].hist(residuals, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
                    "axes[0, 2].axvline(x=0, color='red', linestyle='--', lw=2)\n",
                    "axes[0, 2].set_xlabel('Prediction Error')\n",
                    "axes[0, 2].set_ylabel('Frequency')\n",
                    "axes[0, 2].set_title('Distribution of Prediction Errors')\n",
                    "axes[0, 2].grid(True, alpha=0.3)\n",
                    "\n",
                    "# Plot 4: Accuracy vs Error Threshold\n",
                    "thresholds = [1, 2, 3, 4, 5, 7, 10, 15, 20]\n",
                    "accuracies = [np.mean(np.abs(residuals) <= t) * 100 for t in thresholds]\n",
                    "axes[1, 0].plot(thresholds, accuracies, marker='o', linewidth=3, markersize=8, color='purple')\n",
                    "axes[1, 0].set_xlabel('Error Threshold (points)')\n",
                    "axes[1, 0].set_ylabel('Accuracy (%)')\n",
                    "axes[1, 0].set_title('Accuracy vs Error Threshold')\n",
                    "axes[1, 0].grid(True, alpha=0.3)\n",
                    "axes[1, 0].set_ylim([0, 105])\n",
                    "\n",
                    "# Plot 5: Performance Category Analysis\n",
                    "category_errors = results_df.groupby('performance_category', observed=False)['abs_error'].mean()\n",
                    "colors = ['red', 'orange', 'yellow', 'green']\n",
                    "axes[1, 1].bar(category_errors.index, category_errors.values, color=colors, alpha=0.7, edgecolor='black')\n",
                    "axes[1, 1].set_xlabel('Performance Category')\n",
                    "axes[1, 1].set_ylabel('Average Absolute Error')\n",
                    "axes[1, 1].set_title('Prediction Error by Performance Category')\n",
                    "axes[1, 1].grid(True, alpha=0.3)\n",
                    "\n",
                    "# Plot 6: Prediction Confidence Intervals\n",
                    "error_std = np.std(residuals)\n",
                    "confidence_intervals = [error_std * z for z in [1, 1.96, 2.58]]  # 68%, 95%, 99% confidence\n",
                    "intervals = ['68%', '95%', '99%']\n",
                    "axes[1, 2].bar(intervals, confidence_intervals, color=['lightblue', 'blue', 'darkblue'], alpha=0.7, edgecolor='black')\n",
                    "axes[1, 2].set_xlabel('Confidence Level')\n",
                    "axes[1, 2].set_ylabel('Error Range (¬± points)')\n",
                    "axes[1, 2].set_title('Prediction Confidence Intervals')\n",
                    "axes[1, 2].grid(True, alpha=0.3)\n",
                    "\n",
                    "plt.tight_layout()\n",
                    "plt.show()\n",
                    "\n",
                    "print(\"‚úÖ Visualization dashboard created successfully!\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Additional detailed analysis plots\n",
                    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
                    "fig.suptitle('Detailed Model Performance Analysis', fontsize=14, fontweight='bold')\n",
                    "\n",
                    "# Plot 1: Error vs Actual Performance\n",
                    "axes[0, 0].scatter(y_test, np.abs(residuals), alpha=0.6, color='red', edgecolors='black', linewidth=0.5)\n",
                    "axes[0, 0].set_xlabel('Actual Performance')\n",
                    "axes[0, 0].set_ylabel('Absolute Error')\n",
                    "axes[0, 0].set_title('Error Magnitude vs Actual Performance')\n",
                    "axes[0, 0].grid(True, alpha=0.3)\n",
                    "\n",
                    "# Plot 2: Cumulative Error Distribution\n",
                    "sorted_errors = np.sort(np.abs(residuals))\n",
                    "cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100\n",
                    "axes[0, 1].plot(sorted_errors, cumulative, linewidth=3, color='darkgreen')\n",
                    "axes[0, 1].set_xlabel('Absolute Error (points)')\n",
                    "axes[0, 1].set_ylabel('Cumulative Percentage (%)')\n",
                    "axes[0, 1].set_title('Cumulative Error Distribution')\n",
                    "axes[0, 1].grid(True, alpha=0.3)\n",
                    "axes[0, 1].axhline(y=50, color='red', linestyle='--', alpha=0.7, label='50% of predictions')\n",
                    "axes[0, 1].axhline(y=80, color='orange', linestyle='--', alpha=0.7, label='80% of predictions')\n",
                    "axes[0, 1].legend()\n",
                    "\n",
                    "# Plot 3: Q-Q Plot (Quantile-Quantile)\n",
                    "from scipy import stats\n",
                    "z_scores = stats.zscore(residuals)\n",
                    "theoretical_quantiles = stats.norm.ppf(np.linspace(0.01, 0.99, len(z_scores)))\n",
                    "sample_quantiles = np.sort(z_scores)\n",
                    "axes[1, 0].scatter(theoretical_quantiles, sample_quantiles, alpha=0.6, color='purple', edgecolors='black', linewidth=0.5)\n",
                    "axes[1, 0].plot(theoretical_quantiles, theoretical_quantiles, 'r--', lw=2, label='Perfect Normal Distribution')\n",
                    "axes[1, 0].set_xlabel('Theoretical Quantiles (Normal Distribution)')\n",
                    "axes[1, 0].set_ylabel('Sample Quantiles (Residuals)')\n",
                    "axes[1, 0].set_title('Q-Q Plot: Residual Distribution')\n",
                    "axes[1, 0].legend()\n",
                    "axes[1, 0].grid(True, alpha=0.3)\n",
                    "\n",
                    "# Plot 4: Feature Importance (from model coefficients)\n",
                    "feature_names = ['Age', 'Grade', 'Attendance', 'Current Marks', 'Subject']\n",
                    "coefficients = model.coef_\n",
                    "axes[1, 1].barh(feature_names, coefficients, color='skyblue', edgecolor='black', alpha=0.7)\n",
                    "axes[1, 1].set_xlabel('Coefficient Value')\n",
                    "axes[1, 1].set_title('Feature Importance (Linear Regression Coefficients)')\n",
                    "axes[1, 1].grid(True, alpha=0.3)\n",
                    "axes[1, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
                    "\n",
                    "plt.tight_layout()\n",
                    "plt.show()\n",
                    "\n",
                    "print(\"‚úÖ Detailed analysis plots created successfully!\")"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Create an interactive prediction example\n",
                    "print(\"üéØ INTERACTIVE PREDICTION EXAMPLE\")\n",
                    "print(\"=\"*50)\n",
                    "\n",
                    "# Example student data\n",
                    "example_students = [\n",
                    "    {\n",
                    "        'name': 'High Performer',\n",
                    "        'age': 16,\n",
                    "        'grade': 11,\n",
                    "        'attendance': 95.0,\n",
                    "        'marks': 88.0,\n",
                    "        'subject': 'Mathematics'\n",
                    "    },\n",
                    "    {\n",
                    "        'name': 'Average Student',\n",
                    "        'age': 15,\n",
                    "        'grade': 10,\n",
                    "        'attendance': 75.0,\n",
                    "        'marks': 65.0,\n",
                    "        'subject': 'Science'\n",
                    "    },\n",
                    "    {\n",
                    "        'name': 'Struggling Student',\n",
                    "        'age': 17,\n",
                    "        'grade': 12,\n",
                    "        'attendance': 55.0,\n",
                    "        'marks': 45.0,\n",
                    "        'subject': 'English'\n",
                    "    }\n",
                    "]\n",
                    "\n",
                    "print(\"üìä Predicting performance for example students:\")\n",
                    "print(\"-\"*50)\n",
                    "\n",
                    "for student in example_students:\n",
                    "    # Prepare input data\n",
                    "    subject_encoded = label_encoder.transform([student['subject']])[0]\n",
                    "    input_data = np.array([[\n",
                    "        student['age'],\n",
                    "        student['grade'], \n",
                    "        student['attendance'],\n",
                    "        student['marks'],\n",
                    "        subject_encoded\n",
                    "    ]])\n",
                    "    \n",
                    "    # Scale input\n",
                    "    input_scaled = scaler.transform(input_data)\n",
                    "    \n",
                    "    # Make prediction\n",
                    "    prediction = model.predict(input_scaled)[0]\n",
                    "    \n",
                    "    print(f\"{student['name']:<20} | Actual: {student['marks']:<3} | Predicted: {prediction:.1f}\")\n",
                    "    print(f\"{'':<20} | Age: {student['age']}, Grade: {student['grade']}\")\n",
                    "    print(f\"{'':<20} | Attendance: {student['attendance']}%, Subject: {student['subject']}\")\n",
                    "    print()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "## 6. Analysis and Conclusions\n",
                    "\n",
                    "Summary of model performance and recommendations for improvement."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Create a comprehensive analysis summary\n",
                    "print(\"üìã\" + \"=\"*70)\n",
                    "print(\"MODEL PERFORMANCE ANALYSIS SUMMARY\")\n",
                    "print(\"=\"*70)\n",
                    "\n",
                    "# Performance assessment\n",
                    "if metrics['r2_score'] >= 0.8:\n",
                    "    overall_performance = \"EXCELLENT\"\n",
                    "    color = \"üü¢\"\n",
                    "elif metrics['r2_score'] >= 0.6:\n",
                    "    overall_performance = \"GOOD\"\n",
                    "    color = \"üü°\"\n",
                    "elif metrics['r2_score'] >= 0.3:\n",
                    "    overall_performance = \"MODERATE\"\n",
                    "    color = \"üü†\"\n",
                    "else:\n",
                    "    overall_performance = \"NEEDS IMPROVEMENT\"\n",
                    "    color = \"üî¥\"\n",
                    "\n",
                    "print(f\"\\nüéØ OVERALL MODEL PERFORMANCE: {color} {overall_performance}\")\n",
                    "print(\"-\"*70)\n",
                    "print(f\"  R¬≤ Score: {metrics['r2_score']:.4f} ({metrics['r2_score']*100:.2f}%)\")\n",
                    "print(f\"  Mean Absolute Error: {metrics['mae']:.2f} points\")\n",
                    "print(f\"  Accuracy within ¬±5 points: {metrics['accuracy_within_5']:.1f}%\")\n",
                    "\n",
                    "print(\"\\nüìä STRENGTHS:\")\n",
                    "print(\"-\"*70)\n",
                    "if metrics['accuracy_within_10'] >= 70:\n",
                    "    print(\"  ‚úÖ Good accuracy for rough estimates (within ¬±10 points)\")\n",
                    "if metrics['r2_score'] > 0:\n",
                    "    print(f\"  ‚úÖ Model explains {metrics['r2_score']*100:.1f}% of performance variance\")\n",
                    "else:\n",
                    "    print(\"  ‚ö†Ô∏è  Model performs worse than baseline prediction\")\n",
                    "\n",
                    "print(\"\\n‚ö†Ô∏è  WEAKNESSES:\")\n",
                    "print(\"-\"*70)\n",
                    "if metrics['accuracy_within_5'] < 50:\n",
                    "    print(f\"  ‚ùå Low precision (only {metrics['accuracy_within_5']:.1f}% predictions within ¬±5 points)\")\n",
                    "if metrics['r2_score'] < 0.5:\n",
                    "    print(\"  ‚ùå Model explains less than 50% of performance variance\")\n",
                    "if metrics['mae'] > 5:\n",
                    "    print(f\"  ‚ùå High average error (¬±{metrics['mae']:.1f} points)\")\n",
                    "\n",
                    "print(\"\\nüîß RECOMMENDATIONS FOR IMPROVEMENT:\")\n",
                    "print(\"-\"*70)\n",
                    "print(\"  1. üìà Add more predictive features:\")\n",
                    "print(\"     ‚Ä¢ Homework completion rate\")\n",
                    "print(\"     ‚Ä¢ Study hours per week\")\n",
                    "print(\"     ‚Ä¢ Previous term performance\")\n",
                    "print(\"     ‚Ä¢ Class participation metrics\")\n",
                    "print(\"     ‚Ä¢ Parent involvement indicators\")\n",
                    "print(\"\")\n",
                    "print(\"  2. ü§ñ Try advanced algorithms:\")\n",
                    "print(\"     ‚Ä¢ Random Forest Regressor\")\n",
                    "print(\"     ‚Ä¢ Gradient Boosting (XGBoost, LightGBM)\")\n",
                    "print(\"     ‚Ä¢ Neural Networks\")\n",
                    "print(\"     ‚Ä¢ Ensemble methods\")\n",
                    "print(\"\")\n",
                    "print(\"  3. üìä Improve data quality:\")\n",
                    "print(\"     ‚Ä¢ Collect more diverse student data\")\n",
                    "print(\"     ‚Ä¢ Handle missing values better\")\n",
                    "print(\"     ‚Ä¢ Remove outliers\")\n",
                    "print(\"     ‚Ä¢ Validate data consistency\")\n",
                    "print(\"\")\n",
                    "print(\"  4. üéØ Feature engineering:\")\n",
                    "print(\"     ‚Ä¢ Create interaction features\")\n",
                    "print(\"     ‚Ä¢ Add polynomial features\")\n",
                    "print(\"     ‚Ä¢ Include categorical encodings\")\n",
                    "print(\"     ‚Ä¢ Time-based features\")\n",
                    "\n",
                    "print(\"\\nüìà EXPECTED IMPROVEMENT TARGETS:\")\n",
                    "print(\"-\"*70)\n",
                    "print(f\"  Target R¬≤ Score: >70% (currently {metrics['r2_score']*100:.1f}%)\")\n",
                    "print(f\"  Target MAE: <4.0 points (currently {metrics['mae']:.2f})\")\n",
                    "print(f\"  Target Accuracy ¬±5: >70% (currently {metrics['accuracy_within_5']:.1f}%)\")\n",
                    "\n",
                    "print(\"\\nüß™ HOW TO RETRAIN:\")\n",
                    "print(\"-\"*70)\n",
                    "print(\"  1. Add new features to dataset\")\n",
                    "print(\"  2. Run: python src/data_preprocessing.py\")\n",
                    "print(\"  3. Run: python src/model_trainer.py\")\n",
                    "print(\"  4. Run: python evaluate_model.py\")\n",
                    "print(\"  5. Re-run this notebook for new analysis\")\n",
                    "\n",
                    "print(\"\\n\" + \"=\"*70)\n",
                    "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
                    "print(\"=\"*70)"
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "outputs": [],
                "source": [
                    "# Save results to file for future reference\n",
                    "import json\n",
                    "from datetime import datetime\n",
                    "\n",
                    "results_summary = {\n",
                    "    'evaluation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
                    "    'model_type': 'Linear Regression',\n",
                    "    'dataset_size': len(df),\n",
                    "    'test_size': len(X_test),\n",
                    "    'metrics': metrics,\n",
                    "    'performance_assessment': overall_performance,\n",
                    "    'recommendations': [\n",
                    "        'Add more predictive features (homework completion, study hours, etc.)',\n",
                    "        'Try advanced algorithms (Random Forest, XGBoost)',\n",
                    "        'Improve data quality and quantity',\n",
                    "        'Feature engineering and selection'\n",
                    "    ]\n",
                    "}\n",
                    "\n",
                    "# Save to JSON file\n",
                    "with open('model_accuracy_results.json', 'w') as f:\n",
                    "    json.dump(results_summary, f, indent=2, default=str)\n",
                    "\n",
                    "print(\"üíæ Results saved to: model_accuracy_results.json\")\n",
                    "\n",
                    "# Display final summary\n",
                    "print(\"\\nüéØ FINAL SUMMARY:\")\n",
                    "print(f\"Model Performance: {overall_performance}\")\n",
                    "print(f\"R¬≤ Score: {metrics['r2_score']:.4f} ({metrics['r2_score']*100:.2f}%)\")\n",
                    "print(f\"Mean Absolute Error: {metrics['mae']:.2f} points\")\n",
                    "print(f\"Accuracy within ¬±5 points: {metrics['accuracy_within_5']:.1f}%\")\n",
                    "print(f\"Analysis Date: {results_summary['evaluation_date']}\")"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.5"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }

    # Save the notebook
    with open('Model_Accuracy_Analysis.ipynb', 'w', encoding='utf-8') as f:
        json.dump(notebook_content, f, indent=1, ensure_ascii=False)

    print("‚úÖ Jupyter notebook created successfully!")
    print("üìÅ File: Model_Accuracy_Analysis.ipynb")
    print("\nüöÄ To run the notebook:")
    print("1. Install Jupyter: pip install jupyter")
    print("2. Start Jupyter: jupyter notebook")
    print("3. Open: Model_Accuracy_Analysis.ipynb")
    print("4. Run all cells to see the complete analysis")

if __name__ == "__main__":
    create_notebook()
